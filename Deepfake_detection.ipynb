{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehakcrystal/SOC/blob/main/Deepfake_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Creation"
      ],
      "metadata": {
        "id": "wfE4XuG8WGKK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq2XWI9_4TcN"
      },
      "outputs": [],
      "source": [
        "# ------------------ Kaggle Dataset Download (Optional) ------------------\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle && chmod 600 ~/.kaggle/kaggle.json\n",
        "# Upload kaggle.json manually in Colab: Files > Upload kaggle.json to ~/.kaggle/\n",
        "!kaggle datasets download -d greatgamedota/faceforensics\n",
        "!unzip -q faceforensics.zip -d data/FF\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Upload kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()  # Upload your kaggle.json file here\n",
        "\n",
        "# Step 2: Move to the right location and fix permissions\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Step 3: Install kaggle CLI and download the dataset\n",
        "!pip install -q kaggle\n",
        "!kaggle datasets download -d manjilkarki/deepfake-and-real-images\n",
        "!mkdir -p data/df_real_fake\n",
        "!unzip -q deepfake-and-real-images.zip -d data/df_real_fake\n",
        "\n",
        "# 3. Confirm folder structure\n",
        "!find data/df_real_fake -maxdepth 2 -type d\n",
        "\n",
        "# Optional: Install OpenCV\n",
        "!pip install opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "5GE6H-sw63Tz",
        "outputId": "e102293f-381a-407f-dcdf-d83078d9ef2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-68a66b15-827a-44ba-8810-804478e72657\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-68a66b15-827a-44ba-8810-804478e72657\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Implementation"
      ],
      "metadata": {
        "id": "VXy1T7bxWAwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Imports ------------------\n",
        "import io, numpy as np\n",
        "from PIL import Image\n",
        "import torch, torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import torchvision.models as models\n",
        "#import cv2\n",
        "!pip install opencv-python\n",
        "import cv2\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------ Augmentations ------------------\n",
        "class RandomJPEG:\n",
        "    def __init__(self, quality=(30, 90), p=0.5):\n",
        "        self.quality = quality; self.p = p\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() < self.p:\n",
        "            q = np.random.randint(self.quality[0], self.quality[1])\n",
        "            buffer = io.BytesIO()\n",
        "            img.save(buffer, format='JPEG', quality=q)\n",
        "            img = Image.open(buffer).convert('RGB')\n",
        "        return img\n",
        "\n",
        "class AddGaussianNoise:\n",
        "    def __init__(self, mean=0., std=5., p=0.5):\n",
        "        self.mean = mean; self.std = std; self.p = p\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() < self.p:\n",
        "            arr = np.array(img).astype(np.float32)\n",
        "            noise = np.random.normal(self.mean, self.std, arr.shape)\n",
        "            arr = np.clip(arr + noise, 0, 255)\n",
        "            img = Image.fromarray(arr.astype(np.uint8))\n",
        "        return img\n",
        "\n",
        "train_transforms = T.Compose([\n",
        "    T.RandomResizedCrop(224),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    RandomJPEG(quality=(30, 90), p=0.7),\n",
        "    AddGaussianNoise(std=10, p=0.5),\n",
        "    T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
        "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "val_transforms = T.Compose([\n",
        "    T.Resize(256), T.CenterCrop(224), T.ToTensor()\n",
        "])\n",
        "\n",
        "# ------------------ High-Frequency Transform ------------------\n",
        "class HighFreqTransform:\n",
        "    def __call__(self, pil_img):\n",
        "        arr = np.array(pil_img)\n",
        "        hf = np.zeros_like(arr)\n",
        "        for c in range(3):\n",
        "            channel = cv2.Laplacian(arr[:, :, c], cv2.CV_32F, ksize=3)\n",
        "            hf[:, :, c] = cv2.convertScaleAbs(channel)\n",
        "        return Image.fromarray(hf)\n",
        "\n",
        "hf_transform = HighFreqTransform()\n",
        "\n",
        "# ------------------ DualStreamEncoder ------------------\n",
        "class DualStreamEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.rgb_encoder = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "        self.rgb_encoder.fc = nn.Linear(self.rgb_encoder.fc.in_features, embed_dim)\n",
        "\n",
        "        self.hf_encoder = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "        self.hf_encoder.fc = nn.Linear(self.hf_encoder.fc.in_features, embed_dim)\n",
        "\n",
        "        self.fc_fuse = nn.Sequential(\n",
        "            nn.Linear(2 * embed_dim, embed_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_rgb, x_hf):\n",
        "        feat_rgb = self.rgb_encoder(x_rgb)\n",
        "        feat_hf = self.hf_encoder(x_hf)\n",
        "        feat = torch.cat([feat_rgb, feat_hf], dim=1)\n",
        "        return self.fc_fuse(feat)\n",
        "\n",
        "# ------------------ Prototype Loss ------------------\n",
        "def proto_loss_fn(support_embed, support_labels, query_embed, query_labels):\n",
        "    all_embed = torch.cat([support_embed, query_embed], dim=0)\n",
        "    all_labels = torch.cat([support_labels, query_labels], dim=0)\n",
        "\n",
        "    classes = torch.sort(torch.unique(all_labels))[0]\n",
        "    prototypes = torch.stack([all_embed[all_labels == c].mean(dim=0) for c in classes])\n",
        "\n",
        "    query_labels_mapped = torch.zeros_like(query_labels)\n",
        "    for i, c in enumerate(classes):\n",
        "        query_labels_mapped[query_labels == c] = i\n",
        "\n",
        "    dists = torch.cdist(query_embed, prototypes)\n",
        "    logits = -dists\n",
        "    return F.cross_entropy(logits, query_labels_mapped), logits, query_labels_mapped\n",
        "\n"
      ],
      "metadata": {
        "id": "dqORYdBQCbOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "# Define the to_hf_tensor function outside the loops\n",
        "def to_hf_tensor(batch, hf_transform, device):\n",
        "    return torch.stack([\n",
        "        T.ToTensor()(hf_transform(T.ToPILImage()(img.cpu()))).to(device)\n",
        "        for img in batch\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "def validate_with_prototypes(model, train_loader, val_loader, hf_transform, device):\n",
        "    model.eval()\n",
        "\n",
        "    # --- Build prototypes from training embeddings ---\n",
        "    support_embeds, support_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            hf_imgs = to_hf_tensor(imgs)\n",
        "            emb = model(imgs, hf_imgs)\n",
        "            support_embeds.append(emb)\n",
        "            support_labels.append(labels)\n",
        "    support_embeds = torch.cat(support_embeds)\n",
        "    support_labels = torch.cat(support_labels)\n",
        "\n",
        "    unique_classes = torch.sort(torch.unique(support_labels))[0]\n",
        "    prototypes = torch.stack([\n",
        "        support_embeds[support_labels == c].mean(dim=0) for c in unique_classes\n",
        "    ])\n",
        "\n",
        "    # --- Embed validation images ---\n",
        "    query_embeds, query_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            hf_imgs = to_hf_tensor(imgs)\n",
        "            emb = model(imgs, hf_imgs)\n",
        "            query_embeds.append(emb)\n",
        "            query_labels.append(labels)\n",
        "    query_embeds = torch.cat(query_embeds)\n",
        "    query_labels = torch.cat(query_labels)\n",
        "\n",
        "    # --- Predict using nearest prototype ---\n",
        "    dists = torch.cdist(query_embeds, prototypes)  # shape: [num_val, num_classes]\n",
        "    pred_indices = dists.argmin(dim=1)\n",
        "\n",
        "    # --- Map true labels to prototype class indices ---\n",
        "    mapped_labels = torch.zeros_like(query_labels)\n",
        "    for i, c in enumerate(unique_classes):\n",
        "        mapped_labels[query_labels == c] = i\n",
        "\n",
        "    # --- Accuracy ---\n",
        "    acc = accuracy_score(mapped_labels.cpu().numpy(), pred_indices.cpu().numpy())\n",
        "\n",
        "    # --- ROC-AUC (optional) ---\n",
        "    try:\n",
        "        probs = (-dists).softmax(dim=1)\n",
        "        auc = roc_auc_score(mapped_labels.cpu().numpy(), probs[:, 1].cpu().numpy())\n",
        "    except:\n",
        "        auc = float('nan')\n",
        "\n",
        "    print(f\"Val Accuracy: {acc:.4f} | ROC-AUC: {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "r-nEFaaHavUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ Data & Model Init ------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "import random\n",
        "\n",
        "# Load full dataset\n",
        "full_train_dataset = ImageFolder(root=\"data/df_real_fake/Dataset/Train\", transform=train_transforms)\n",
        "full_val_dataset = ImageFolder(root=\"data/df_real_fake/Dataset/Validation\", transform=val_transforms)\n",
        "\n",
        "# Select small subset (e.g., 20 images for train, 20 for val)\n",
        "train_indices = random.sample(range(len(full_train_dataset)), min(100, len(full_train_dataset)))\n",
        "\n",
        "val_indices = random.sample(range(len(full_val_dataset)), min(20, len(full_val_dataset)))\n",
        "\n",
        "# Create subset datasets\n",
        "train_dataset = Subset(full_train_dataset, train_indices)\n",
        "val_dataset = Subset(full_val_dataset, val_indices)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "model = DualStreamEncoder().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ------------------ Training Loop ------------------\n",
        "num_epochs = 2  # Just for testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4)\n",
        "support_per_class = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, step_count = 0.0, 0\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    for imgs, labels in tqdm(train_loader):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        if len(imgs) <= support_per_class * 2:\n",
        "            print(\"Skipping small batch.\")\n",
        "            continue\n",
        "\n",
        "        unique_classes = labels.unique()\n",
        "        support_idxs, query_idxs = [], []\n",
        "\n",
        "        for cls in unique_classes:\n",
        "            cls_idxs = (labels == cls).nonzero(as_tuple=True)[0]\n",
        "            if len(cls_idxs) < support_per_class + 1:\n",
        "                continue\n",
        "            cls_idxs = cls_idxs[torch.randperm(len(cls_idxs))]\n",
        "            support_idxs += cls_idxs[:support_per_class].tolist()\n",
        "            query_idxs += cls_idxs[support_per_class:].tolist()\n",
        "\n",
        "        if len(support_idxs) == 0 or len(query_idxs) == 0:\n",
        "            print(\"Skipping batch due to class imbalance.\")\n",
        "            continue\n",
        "\n",
        "        support_imgs = imgs[support_idxs]\n",
        "        support_labels = labels[support_idxs]\n",
        "        query_imgs = imgs[query_idxs]\n",
        "        query_labels = labels[query_idxs]\n",
        "\n",
        "        # High-frequency tensors\n",
        "        '''def to_hf_tensor(batch):\n",
        "            return torch.stack([\n",
        "                T.ToTensor()(hf_transform(T.ToPILImage()(img.cpu()))).to(device)\n",
        "                for img in batch\n",
        "            ])'''\n",
        "\n",
        "        # OVERWRITE any previous version\n",
        "        def to_hf_tensor(batch, hf_transform= hf_transform, device= device):\n",
        "          import torchvision.transforms as T\n",
        "          from PIL import Image\n",
        "          import torch\n",
        "          return torch.stack([\n",
        "            T.ToTensor()(hf_transform(T.ToPILImage()(img.cpu()))).to(device)\n",
        "            for img in batch\n",
        "            ])\n",
        "\n",
        "\n",
        "        support_hf = to_hf_tensor(support_imgs, hf_transform, device)\n",
        "        query_hf = to_hf_tensor(query_imgs, hf_transform, device)\n",
        "\n",
        "        support_embed = model(support_imgs, support_hf)\n",
        "        query_embed = model(query_imgs, query_hf)\n",
        "\n",
        "        loss, logits, q_mapped = proto_loss_fn(support_embed, support_labels, query_embed, query_labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        step_count += 1\n",
        "\n",
        "    avg_loss = total_loss / step_count if step_count > 0 else 0\n",
        "    print(f\"Train Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "krrSkUvHazm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate_with_prototypes(model, train_loader, val_loader, hf_transform, device)"
      ],
      "metadata": {
        "id": "vepmrTKHC_tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Grad-CAM Function ----\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def show_gradcam(model, img_tensor, hf_tensor, device= device, class_idx=None, layer_name=\"layer4\"):\n",
        "    model.eval()\n",
        "\n",
        "    activations, gradients = {}, {}\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        activations['value'] = output\n",
        "\n",
        "    def backward_hook(module, grad_input, grad_output):\n",
        "        gradients['value'] = grad_output[0]\n",
        "\n",
        "    # Hook into RGB encoderâ€™s layer4\n",
        "    target_layer = dict(model.rgb_encoder.named_modules())[layer_name]\n",
        "    fwd_handle = target_layer.register_forward_hook(forward_hook)\n",
        "    bwd_handle = target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    # Forward pass\n",
        "    img_tensor = img_tensor.unsqueeze(0)  # add batch dimension\n",
        "    hf_tensor = hf_tensor.unsqueeze(0)\n",
        "    img_tensor, hf_tensor = img_tensor.to(device), hf_tensor.to(device)\n",
        "    output = model(img_tensor, hf_tensor)\n",
        "    pred_class = output.argmax(dim=1).item() if class_idx is None else class_idx\n",
        "\n",
        "    # Backward pass\n",
        "    model.zero_grad()\n",
        "    class_score = output[0, pred_class]\n",
        "    class_score.backward()\n",
        "\n",
        "    grads = gradients['value'][0].detach().cpu().numpy()\n",
        "    acts = activations['value'][0].detach().cpu().numpy()\n",
        "\n",
        "    weights = np.mean(grads, axis=(1, 2))\n",
        "    cam = np.sum(weights[:, None, None] * acts, axis=0)\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cv2.resize(cam, (224, 224))\n",
        "    cam = (cam - cam.min()) / (cam.max() + 1e-8)\n",
        "\n",
        "    orig = img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "    orig = (orig - orig.min()) / (orig.max() - orig.min() + 1e-8)\n",
        "\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "    overlay = heatmap * 0.4 + np.uint8(255 * orig)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.imshow(orig)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Grad-CAM\")\n",
        "    plt.imshow(np.uint8(overlay))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    fwd_handle.remove()\n",
        "    bwd_handle.remove()\n"
      ],
      "metadata": {
        "id": "7QAo3xeIa9_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample image + its HF version, compatible with your pipeline\n",
        "sample_img, _ = val_dataset[0]  # from your Subset\n",
        "hf_img = to_hf_tensor(sample_img.unsqueeze(0), hf_transform, device)[0]\n",
        "\n",
        "# Visualize Grad-CAM\n",
        "show_gradcam(model, sample_img, hf_img, device)\n"
      ],
      "metadata": {
        "id": "DNbGe103bcLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_tsne(model, loader, hf_transform, device):\n",
        "    model.eval()\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in tqdm(loader):\n",
        "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "            hf_imgs = to_hf_tensor(imgs)\n",
        "            embs = model(imgs, hf_imgs)\n",
        "            embeddings.append(embs.cpu())\n",
        "            labels.append(lbls.cpu())\n",
        "\n",
        "    embeddings = torch.cat(embeddings).numpy()\n",
        "    labels = torch.cat(labels).numpy()\n",
        "\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
        "    reduced = tsne.fit_transform(embeddings)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=reduced[:, 0], y=reduced[:, 1], hue=labels, palette='coolwarm', s=50)\n",
        "    plt.title(\"t-SNE of Embeddings (Real vs Fake)\")\n",
        "    plt.legend(title=\"Class\", labels=[\"Real\", \"Fake\"])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Gr2bTxmfjHMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tsne(model, val_loader, hf_transform, device)\n"
      ],
      "metadata": {
        "id": "QrW7XoJRjK4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn\n",
        "\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_umap(model, loader, hf_transform, device):\n",
        "    model.eval()\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in tqdm(loader):\n",
        "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "            hf_imgs = to_hf_tensor(imgs)  # default args used\n",
        "            embs = model(imgs, hf_imgs)\n",
        "            embeddings.append(embs.cpu())\n",
        "            labels.append(lbls.cpu())\n",
        "\n",
        "    embeddings = torch.cat(embeddings).numpy()\n",
        "    labels = torch.cat(labels).numpy()\n",
        "\n",
        "    # Run UMAP\n",
        "    umap = UMAP(n_components=2, random_state=42)\n",
        "    reduced = umap.fit_transform(embeddings)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=reduced[:, 0], y=reduced[:, 1], hue=labels, palette='coolwarm', s=50)\n",
        "    plt.title(\"UMAP of Embeddings (Real vs Fake)\")\n",
        "    plt.legend(title=\"Class\", labels=[\"Real\", \"Fake\"])\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g6biiUQkjUTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_umap(model, val_loader, hf_transform, device)\n"
      ],
      "metadata": {
        "id": "cyjzW3rqkPIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_img = RandomJPEG(p=1.0)(T.ToPILImage()(sample_img.cpu()))\n",
        "aug_tensor = T.ToTensor()(aug_img).to(device)\n",
        "aug_hf_tensor = to_hf_tensor(aug_tensor.unsqueeze(0))[0]\n",
        "\n",
        "show_gradcam(model, aug_tensor, aug_hf_tensor)"
      ],
      "metadata": {
        "id": "AFLfJ7x1kZQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Get the image file path from ImageFolder\n",
        "img_path, label = full_val_dataset.samples[0]  # This returns (path, class_index)\n",
        "\n",
        "# Now load it as a proper PIL Image\n",
        "orig_img_pil = Image.open(img_path).convert('RGB')  # THIS IS PIL NOW\n"
      ],
      "metadata": {
        "id": "KifafIn974LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_img_tensor = train_transforms(orig_img_pil)  # This works now\n"
      ],
      "metadata": {
        "id": "Bge0YpQG8Adv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "\n",
        "augmented_img_pil = T.ToPILImage()(augmented_img_tensor)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(orig_img_pil)\n",
        "plt.title(\"Original\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(augmented_img_pil)\n",
        "plt.title(\"Augmented\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "fPqUpNq_luc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Load a sample from dataset (e.g., from val_dataset)\n",
        "# Load without transform\n",
        "from PIL import Image\n",
        "orig_img_path = full_val_dataset.samples[0][0]  # path to image\n",
        "orig_img_pil = Image.open(orig_img_path).convert('RGB')\n",
        " # This gives PIL image because transform is applied later\n",
        "\n",
        "# Apply your train-time augmentations (simulate one augmentation)\n",
        "augmented_img_tensor = train_transforms(orig_img_pil)\n",
        "augmented_img_pil = T.ToPILImage()(augmented_img_tensor)\n",
        "\n",
        "# Show both images side by side\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(orig_img_pil)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(augmented_img_pil)\n",
        "plt.title(\"Augmented Image\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eHwyZEPkmHA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiGranularDualStreamEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=256):\n",
        "        super().__init__()\n",
        "        base_rgb = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "        base_hf = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "        self.rgb_backbone = nn.Sequential(*list(base_rgb.children())[:-2])  # up to layer4\n",
        "        self.hf_backbone = nn.Sequential(*list(base_hf.children())[:-2])\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)  # global feature\n",
        "        self.local_conv = nn.Conv2d(512, embed_dim, kernel_size=1)  # local-level\n",
        "\n",
        "        self.fc_fuse = nn.Sequential(\n",
        "            nn.Linear(2 * (embed_dim + embed_dim), embed_dim),  # global + local from each stream\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_rgb, x_hf):\n",
        "        feat_rgb = self.rgb_backbone(x_rgb)\n",
        "        feat_hf = self.hf_backbone(x_hf)\n",
        "\n",
        "        global_rgb = self.pool(feat_rgb).view(x_rgb.size(0), -1)\n",
        "        global_hf = self.pool(feat_hf).view(x_hf.size(0), -1)\n",
        "\n",
        "        local_rgb = self.pool(self.local_conv(feat_rgb)).view(x_rgb.size(0), -1)\n",
        "        local_hf = self.pool(self.local_conv(feat_hf)).view(x_hf.size(0), -1)\n",
        "\n",
        "        combined = torch.cat([global_rgb, local_rgb, global_hf, local_hf], dim=1)\n",
        "        return self.fc_fuse(combined)\n"
      ],
      "metadata": {
        "id": "6UUmr9q95_jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Step 1: Load raw PIL image\n",
        "img_path, _ = full_val_dataset.samples[0]\n",
        "pil_img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "# Step 2: Apply transforms to get input tensor\n",
        "img_tensor = val_transforms(pil_img).to(device)\n",
        "\n",
        "# Step 3: Convert to HF tensor\n",
        "hf_tensor = to_hf_tensor(img_tensor.unsqueeze(0), hf_transform, device)[0]\n"
      ],
      "metadata": {
        "id": "GiSohegU8jgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_gradcam(model, img_tensor, hf_tensor, layer_name=\"layer2\")  # Local\n",
        "show_gradcam(model, img_tensor, hf_tensor, layer_name=\"layer4\")  # Global\n"
      ],
      "metadata": {
        "id": "J6AjUJ2p8YSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiGranularMaskHead(nn.Module):\n",
        "    def __init__(self, in_channels=512, out_channels=2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, out_channels, kernel_size=1)  # 2 masks\n",
        "        )\n",
        "\n",
        "    def forward(self, feat_map):\n",
        "        return self.conv(feat_map)  # shape: [B, 2, H, W]\n"
      ],
      "metadata": {
        "id": "qZ2atEog-CAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_bigranularity_masks(input_imgs, m_in_gt, m_in_pred, m_ex_gt, m_ex_pred):\n",
        "    n = len(input_imgs)\n",
        "    plt.figure(figsize=(12, n * 2))\n",
        "\n",
        "    for i in range(n):\n",
        "        imgs = [input_imgs[i], m_in_gt[i], m_in_pred[i], m_ex_gt[i], m_ex_pred[i]]\n",
        "        for j, img in enumerate(imgs):\n",
        "            plt.subplot(n, 5, i * 5 + j + 1)\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                img = img.detach().cpu().numpy()\n",
        "                if img.ndim == 3 and img.shape[0] == 1:\n",
        "                    img = img.squeeze(0)  # grayscale mask\n",
        "                elif img.shape[0] == 3:\n",
        "                    img = img.transpose(1, 2, 0)  # RGB\n",
        "            plt.imshow(img, cmap='gray' if j > 0 else None)\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "jfpYUInh-VFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiGranularityDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_size=(224, 224)):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 2, 1),  # Output 2 masks\n",
        "        )\n",
        "        self.upsample = nn.Upsample(size=out_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "    def forward(self, feat):\n",
        "        x = self.conv(feat)\n",
        "        return self.upsample(x)  # [B, 2, H, W]\n"
      ],
      "metadata": {
        "id": "kekIlncJ-iVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "Soec5kA5fhXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "id": "Mmfg9ZyZhVnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile BiGranularDualStreamEncoder.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BiGranularDualStreamEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BiGranularDualStreamEncoder, self).__init__()\n",
        "        # Define both RGB and high-frequency (HF) processing layers\n",
        "        self.rgb_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.hf_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Fusion layer\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Two mask prediction heads\n",
        "        self.mask_in_head = nn.Conv2d(64, 1, kernel_size=1)\n",
        "        self.mask_ex_head = nn.Conv2d(64, 1, kernel_size=1)\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, rgb, hf):\n",
        "        rgb_feat = self.rgb_conv(rgb)\n",
        "        hf_feat = self.hf_conv(hf)\n",
        "        fused = self.fusion(torch.cat([rgb_feat, hf_feat], dim=1))\n",
        "        mask_in = torch.sigmoid(self.mask_in_head(fused))\n",
        "        mask_ex = torch.sigmoid(self.mask_ex_head(fused))\n",
        "        cls = self.classifier(fused)\n",
        "        return mask_in, mask_ex, cls\n",
        "\n"
      ],
      "metadata": {
        "id": "rNprwRIokh7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile BiGranularDualStreamEncoder.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BiGranularDualStreamEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BiGranularDualStreamEncoder, self).__init__()\n",
        "\n",
        "        # RGB stream\n",
        "        self.rgb_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # High-Frequency stream\n",
        "        self.hf_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Fusion\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Bi-Granular Heads\n",
        "        self.mask_in_head = nn.Conv2d(32, 1, kernel_size=1)\n",
        "        self.mask_ex_head = nn.Conv2d(32, 1, kernel_size=1)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, rgb, hf):\n",
        "        rgb_feat = self.rgb_conv(rgb)\n",
        "        hf_feat = self.hf_conv(hf)\n",
        "        fused = self.fusion(torch.cat([rgb_feat, hf_feat], dim=1))\n",
        "\n",
        "        mask_in = torch.sigmoid(self.mask_in_head(fused))\n",
        "        mask_ex = torch.sigmoid(self.mask_ex_head(fused))\n",
        "        cls = self.classifier(fused)\n",
        "\n",
        "        return mask_in, mask_ex, cls\n"
      ],
      "metadata": {
        "id": "kFrNXOzGlmhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat BiGranularDualStreamEncoder.py\n"
      ],
      "metadata": {
        "id": "pjkeBhjsmflT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile my_model_file.py\n",
        "# (paste the same class content as above here)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BiGranularDualStreamEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rgb_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.hf_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.mask_in_head = nn.Conv2d(32, 1, kernel_size=1)\n",
        "        self.mask_ex_head = nn.Conv2d(32, 1, kernel_size=1)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, rgb, hf):\n",
        "        rgb_feat = self.rgb_conv(rgb)\n",
        "        hf_feat = self.hf_conv(hf)\n",
        "        fused = self.fusion(torch.cat([rgb_feat, hf_feat], dim=1))\n",
        "\n",
        "        mask_in = torch.sigmoid(self.mask_in_head(fused))\n",
        "        mask_ex = torch.sigmoid(self.mask_ex_head(fused))\n",
        "        cls = self.classifier(fused)\n",
        "\n",
        "        return mask_in, mask_ex, cls\n"
      ],
      "metadata": {
        "id": "vyjvfzdhmxgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from my_model_file import BiGranularDualStreamEncoder\n",
        "model = BiGranularDualStreamEncoder()\n"
      ],
      "metadata": {
        "id": "oBqyhXGmnA5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv BiGranularDualStreamEncoder.py BiGranularDualStreamEncoder.py\n"
      ],
      "metadata": {
        "id": "9ZbbtR6PhZPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiGranularDualStreamEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BiGranularDualStreamEncoder, self).__init__()\n",
        "        # Dummy model for testing, replace with your actual architecture\n",
        "        self.conv = nn.Conv2d(6, 3, kernel_size=3, padding=1)  # 3 RGB + 3 HF = 6 channels input\n",
        "        self.classifier = nn.Linear(3 * 224 * 224, 3)  # flatten for classification\n",
        "\n",
        "    def forward(self, rgb, hf):\n",
        "        x = torch.cat((rgb, hf), dim=1)  # Concatenate along channel dimension\n",
        "        x = self.conv(x)\n",
        "        mask_in = x\n",
        "        mask_ex = x\n",
        "        cls_pred = self.classifier(x.view(x.size(0), -1))  # Flatten before classification\n",
        "        return mask_in, mask_ex, cls_pred\n"
      ],
      "metadata": {
        "id": "kyTbeDJoo3zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from BiGranularDualStreamEncoder import BiGranularDualStreamEncoder\n",
        "\n",
        "model = BiGranularDualStreamEncoder()\n",
        "model.eval()\n",
        "\n",
        "# Dummy inputs\n",
        "img_tensor = torch.randn(1, 3, 224, 224)  # RGB\n",
        "hf_tensor = torch.randn(1, 3, 224, 224)   # High-freq\n",
        "\n",
        "# Run\n",
        "with torch.no_grad():\n",
        "    mask_in_pred, mask_ex_pred, cls_pred = model(img_tensor, hf_tensor)\n",
        "\n",
        "print(\"mask_in_pred shape:\", mask_in_pred.shape)\n",
        "print(\"mask_ex_pred shape:\", mask_ex_pred.shape)\n",
        "print(\"cls_pred shape:\", cls_pred.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AMToAWn_-jci"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}